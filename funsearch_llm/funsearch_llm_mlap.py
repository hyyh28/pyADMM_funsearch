import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
import admm_utils


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int):
        super().__init__(samples_per_prompt)
        additional_prompt = ('Please modify the ADMM optimizer in the provided mlap function to incorporate an unfixed penalty parameter. Specifically, the penalty parameter rho should be adjusted dynamically during each iteration based on the current optimization state, rather than being fixed. Ensure that the updated rho is used correctly in the updates for X, Z, Y1, and Y2.'
                             'Only output the Python code, no descriptions.')
        self._additional_prompt = additional_prompt

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]

    def _draw_sample(self, content: str) -> str:
        prompt = '\n'.join([content, self._additional_prompt])
        while True:
            try:
                conn = http.client.HTTPSConnection("api.deepseek.com")

                payload = json.dumps({
                    "messages": [
                        {
                            "content": "You are a helpful programmer who is familar with python and Admm in optimization. ",
                            "role": "system"
                        },
                        {
                            "content": prompt,
                            "role": "user"
                        }
                    ],
                    "model": "deepseek-coder",
                    "frequency_penalty": 0,
                    "max_tokens": 2048,
                    "presence_penalty": 0,
                    "stop": None,
                    "stream": False,
                    "temperature": 1,
                    "top_p": 1,
                    "logprobs": False,
                    "top_logprobs": None
                })
                headers = {
                    'Authorization': 'Bearer sk-66575172e83e40b2bbcaa1cf6b9f0ae8',
                    'Content-Type': 'application/json',
                    'Accept': 'application/json',
                }
                conn.request("POST", "/chat/completions", payload, headers)
                res = conn.getresponse()
                data = res.read().decode("utf-8")
                data = json.loads(data)
                print(data)
                response = data['choices'][0]['message']['content']
                return response
            except Exception:
                continue


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=False):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate

    def run(
            self,
            program: str,
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        dataset = inputs[test_input]
        result_queue = multiprocessing.Queue()
        process = multiprocessing.Process(
            target=self._compile_and_run_function,
            args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        )
        process.start()
        process.join(timeout=timeout_seconds)
        if process.is_alive():
            # if the process is not finished in time, we consider the program illegal
            process.terminate()
            process.join()
            results = None, False
        else:
            if not result_queue.empty():
                results = result_queue.get_nowait()
            else:
                results = None, False

        if self._verbose:
            print(f'================= Evaluated Program =================')
            program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
            func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
            function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
            function_: str = str(function_).strip('\n')
            print(f'{function_}')
            print(f'-----------------------------------------------------')
            print(f'Score: {str(results)}')
            print(f'=====================================================')
            print(f'\n\n')

        return results

    def _compile_and_run_function(self, program, function_to_run, function_to_evolve, dataset, numba_accelerate,
                                  result_queue):
        try:
            # optimize the code (decorate function_to_run with @numba.jit())
            if numba_accelerate:
                program = evaluator_accelerate.add_numba_decorator(
                    program=program,
                    function_to_evolve=function_to_evolve
                )
            # compile the program, and maps the global func/var/class name to its address
            all_globals_namespace = {}
            # execute the program, map func/var/class to global namespace
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
            # the results must be int or float
            if not isinstance(results, (int, float)):
                result_queue.put((None, False))
                return
            result_queue.put((results, True))
        except:
            # if raise any exception, we assume the execution failed
            result_queue.put((None, False))

specification = r'''
# Proximal operator for nuclear norm
def prox_nuclear(B, lambda_):
    U, S, Vt = np.linalg.svd(B, full_matrices=False)
    S = np.maximum(S - lambda_, 0)
    svp = np.sum(S > 0)
    if svp >= 1:
        S = S[:svp]
        X = U[:, :svp] @ np.diag(S) @ Vt[:svp, :]
        nuclearnorm = np.sum(S)
    else:
        X = np.zeros_like(B)
        nuclearnorm = 0
    return X, nuclearnorm

# Proximal operator for L1 norm
def prox_l1(b, lambda_):
    return np.maximum(0, b - lambda_) + np.minimum(0, b + lambda_)

# Proximal operator for L21 norm
def prox_l21(B, lambda_):
    X = np.zeros_like(B)
    for i in range(B.shape[1]):
        nxi = np.linalg.norm(B[:, i])
        if nxi > lambda_:
            X[:, i] = (1 - lambda_ / nxi) * B[:, i]
    return X

# Proximal operator for tensor l21-norm
def prox_tensor_l21(B, lambda_):
    n1, n2, n3 = B.shape
    X = np.zeros_like(B)
    for i in range(n1):
        for j in range(n2):
            v = B[i, j, :]
            nxi = np.linalg.norm(v)
            if nxi > lambda_:
                X[i, j, :] = (1 - lambda_ / nxi) * v
    return X

# Function to compute the loss based on the selected type
def comp_loss(E, loss):
    if loss == 'l1':
        return np.sum(np.abs(E))
    elif loss == 'l21':
        return np.sum(np.linalg.norm(E, axis=0))
    elif loss == 'l2':
        return 0.5 * np.linalg.norm(E, 'fro')**2
    else:
        raise ValueError("Unsupported loss function")

@funsearch.evolve
def mlap(X, lambda_, alpha, opts):
    tol = opts.get('tol', 1e-8)
    max_iter = opts.get('max_iter', 500)
    rho = opts.get('rho', 1.1)
    mu = opts.get('mu', 1e-4)
    max_mu = opts.get('max_mu', 1e10)
    DEBUG = opts.get('DEBUG', 0)
    loss = opts.get('loss', 'l21')

    d, n, K = X.shape
    Z = np.zeros((n, n, K))
    E = np.zeros((d, n, K))
    J = np.zeros_like(Z)
    S = np.zeros_like(Z)
    Y = np.zeros_like(E)
    W = np.zeros_like(Z)
    V = np.zeros_like(Z)
    XtX = np.zeros((n, n, K))
    invXtXI = np.zeros((n, n, K))
    I = np.eye(n)

    for i in range(K):
        XtX[:, :, i] = X[:, :, i].T @ X[:, :, i]
        invXtXI[:, :, i] = np.linalg.inv(XtX[:, :, i] + I)

    nuclearnormJ = np.zeros(K)

    for iter in range(max_iter):
        Zk = Z.copy()
        Ek = E.copy()
        Jk = J.copy()
        Sk = S.copy()

        # First super block {J, S}
        for i in range(K):
            J[:, :, i], nuclearnormJ[i] = prox_nuclear(Z[:, :, i] + W[:, :, i] / mu, 1 / mu)
            S[:, :, i] = invXtXI[:, :, i] @ (XtX[:, :, i] - X[:, :, i].T @ (E[:, :, i] - Y[:, :, i] / mu) + Z[:, :, i] + (V[:, :, i] - W[:, :, i]) / mu)

        # Second super block {Z, E}
        Z = prox_tensor_l21((J + S - (W + V) / mu) / 2, alpha / (2 * mu))

        XmXS = np.zeros_like(E)
        for i in range(K):
            XmXS[:, :, i] = X[:, :, i] - X[:, :, i] @ S[:, :, i]

        if loss == 'l1':
            for i in range(K):
                E[:, :, i] = prox_l1(XmXS[:, :, i] + Y[:, :, i] / mu, lambda_ / mu)
        elif loss == 'l21':
            for i in range(K):
                E[:, :, i] = prox_l21(XmXS[:, :, i] + Y[:, :, i] / mu, lambda_ / mu)
        elif loss == 'l2':
            for i in range(K):
                E[:, :, i] = (XmXS[:, :, i] + Y[:, :, i] / mu) / (lambda_ / mu + 1)
        else:
            raise ValueError('Unsupported loss function')

        dY = XmXS - E
        dW = Z - J
        dV = Z - S

        chgZ = np.max(np.abs(Zk - Z))
        chgE = np.max(np.abs(Ek - E))
        chgJ = np.max(np.abs(Jk - J))
        chgS = np.max(np.abs(Sk - S))
        chg = np.max([chgZ, chgE, chgJ, chgS, np.max(np.abs(dY)), np.max(np.abs(dW)), np.max(np.abs(dV))])

        if DEBUG:
            if iter == 0 or iter % 10 == 0:
                obj = np.sum(nuclearnormJ) + lambda_ * comp_loss(E, loss) + alpha * comp_loss(Z, 'l21')
                err = np.sqrt(np.linalg.norm(dY)**2 + np.linalg.norm(dW)**2 + np.linalg.norm(dV)**2)
                print(f'iter {iter + 1}, mu={mu}, obj={obj}, err={err}')

        if chg < tol:
            break

        Y += mu * dY
        W += mu * dW
        V += mu * dV
        mu = min(rho * mu, max_mu)

    obj = np.sum(nuclearnormJ) + lambda_ * comp_loss(E, loss) + alpha * comp_loss(Z, 'l21')
    err = np.sqrt(np.linalg.norm(dY)**2 + np.linalg.norm(dW)**2 + np.linalg.norm(dV)**2)

    return Z, E, obj, err, iter

@funsearch.run
def evaluate(instances: dict) -> float:
    # Generate toy data
    opts = instances['opts']
    n1 = opts['n1']
    n2 = opts['n2']
    K = opts['K']
    X = np.random.randn(n1, n2, K)
    lambda_ = opts['lambda_']
    alpha = opts['alpha']
    # Perform MLAP
    Z, E, obj, err, iter = mlap(X, lambda_, alpha, opts)
    print(f'Iterations: {iter}, Objective: {obj}, Error: {err}')
    return -iter
'''

# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config = config.Config(samples_per_prompt=4)
    admm_mlap_config = admm_utils.datasets['mlap']
    global_max_sample_num = 10
    # if it is set to None, funsearch will execute an endless loop
    funsearch.main(
        specification=specification,
        inputs=admm_mlap_config,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='logs/low_rank_matrix_models/mlap/funsearch_mlap',
        temperature=1
    )